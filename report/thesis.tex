\documentclass[11pt,a4paper]{report}

% part of template
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[all]{xypic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{array}
\usepackage{todonotes}
\usepackage{listings}
\usepackage[a4paper]{geometry}

% my packages
% \usepackage{minted}
\usepackage{biblatex}
\usepackage{amssymb}
\addbibresource{bib/thesis.bib}

\def\ci{\perp\!\!\!\perp}
% \def\space{\!\!\!}
\lstset{language=Python}

\makeatletter %otherwise geometry resets everything
\Gm@restore@org
\makeatother

\setlength{\itemsep}{0cm}
\setlength{\voffset}{0cm}
\setlength{\headheight}{0cm}
\setlength{\topmargin}{0cm}
\setlength{\extrarowheight}{3pt} %for superscripts in tabular
\setlength{\arraycolsep}{4pt}
\lstset{basicstyle = \footnotesize, breaklines = true}

\graphicspath{{imgs/}}

\begin{document}
\begin{titlepage}
\begin{center}
\textsc{\LARGE Bachelor thesis\\Computing Science}\\[1.5cm]
\includegraphics[height=100pt]{logo}

\vspace{0.4cm}
\textsc{\Large Radboud University}\\[1cm]
\hrule
\vspace{0.4cm}
\textbf{\huge Bayesian Constraint-based Causal Discovery with Greedy
  Search}\\[0.4cm]
\hrule
\vspace{2cm}
\begin{minipage}[t]{0.45\textwidth}
\begin{flushleft} \large
\textit{Author:}\\
Daan Spijkers\\
s1011382
\end{flushleft}
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
\begin{flushright} \large
\textit{First supervisor/assessor:}\\
dr. Tom Claassen \\
\texttt{tomc@cs.ru.nl}\\[1.3cm]
% \textit{[Second supervisor:]}\\
% title, name\\
% \texttt{e-mail adress}\\[1.3cm]
\textit{Second assessor:}\\
title, name\\
\texttt{e-mail adress}
\end{flushright}
\end{minipage}
\vfill
{\large \today}
\end{center}
\end{titlepage}

% The abstract of your thesis is a brief description of the research hypothesis,
% scientific context, motivation, and results.
% The preferred size of an abstract is one paragraph or one page of text.
\begin{abstract}
How much can we improve the accuracy of the resulting PAG from the BCCD
algorithm using a greedy MAG search to optimise its probabilistic causal
statements?
\end{abstract}

\tableofcontents

% The introduction of your bachelor thesis introduces the research area, the
% research hypothesis, and the scientific contributions of your work.
% A good narrative structure is the one suggested by Simon Peyton Jones
% \begin{itemize}
% \item describe the problem / research question
% \item motivate why this problem must be solved
% \item demonstrate that a (new) solution is needed
% \item explain the intuition behind your solution
% \item motivate why / how your solution solves the problem (this is technical)
% \item explain how it compares with related work
% \end{itemize}
% Close the introduction with a paragraph in which the content of the next chapters
% is briefly mentioned (one sentence per chapter).
\chapter{Introduction}\label{introduction}
Causal inference consists of taking a system of statistical
independencies, and mining a system of causal relations. These causal
relations we then represent in a causal graph.

In an ideal situation, we have a statistical test that determines
whether $X$ and $Y$ are independent with 100\% accuracy. Given such a
perfect test, complete algorithms exist; they give the total causal
information possible from that system. Unfortunately, in the real world
100\% accuracy is not possible, and we often have to make do with
insufficient data.

That is why taking realistic data, and optimizing the result is a relevant
problem. It is not always clear how an algorithm performs in these
situations, even if it is complete. One such complete algorithm is
BCCD\cite{claassenBayesianApproachConstraint2012}, which uses a bayesian
score to return a more robust and informative result than comparable
procedures.

BCCD uses a score-based method to mine a system of logical statements of
the following form:
\begin{align*}
  X \Rightarrow Y \lor X \Rightarrow Z \\
  X \not \Rightarrow Y \land X \not \Rightarrow Z
\end{align*}
That is: information about cause or non-cause between different stochastic
variables. These statements have a particular accuracy associated with
them, representing how sure BCCD is about it. BCCD then uses causal
inference to deduce statements that can be mapped into a PAG. However,
since the data can contain inconsistencies, so can the statements. As of
currently, the way inconsistencies are solved is by using statements in
ascending accuracy, thereby never overwriting information we are more sure
of.

However, it is quite possible that one higher accuracy statement, say
$p_1 = 0.9$ is contradicted by two slightly lower accuracy statements,
say $p_2, p_3 = 0.85$. A simple estimation the likelihood of these
situations is:
\begin{align*}
  p_1 * (1 - p_2) * (1 - p_3) &= 0.9 * (1 - 0.85) * (1 - 0.85) &= 0.02 \\
  (1 - p_1) * p_2 * p_3 &= 0.1 * 0.85 * 0.85 &= 0.07
\end{align*}
It is clear then, that we would rather have the second situation, where
the two lower accuracy statements are true, and that there is something to
be gained here.

An initial impulse is to improve the outcome by cleverly choosing which
statements we do use to construct the graph, and which we do not. This
ends up being a very difficult problem however, and instead we will work
the other way around. That is, by starting at the graph.

We take the initial output of BCCD, as well as its logical statements, and
then iteratively search for better graphs until there is no further
improvement possible. Although doing this greedily means this is likely to
end up in a local optimum, the score must end up higher or equal to the
initial BCCD result.

By doing this we will be using global information to complement the local
information, hopefully getting a better result. To easy scoring we will
first convert the BCCD PAG to a MAG, and then work on that MAG. As we will
see, this conversion also improves the result significantly, presumably
because this incorporates global information that the original algorithm
did not.

To summarize, our contributions are as follows:
\begin{enumerate}
  \item An improved result on the BCCD framework by converting to and from
    a MAG.

  \item A greedy step which takes the MAG and BCCD statements, and returns
    a higher scoring MAG.
\end{enumerate}

% This \emph{optional} chapter contains the stuff that your reader needs
% to know in order to understand your work. Your ``audience" consists of
% fellow third year computing science bachelor students who have done the
% same core courses as you have, but not necessarily the same
% specialization, minor, or free electives.
\chapter{Preliminaries}\label{preliminaries}
\section{Causal Discovery by Example}
In this section we will adapt the example given in
\cite{zhangCompletenessOrientationRules2008}, and give a quick
introduction to DAGs and causal discovery in general.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{imgs/example1.png}
  \caption{Example showing both latent cofounders and selection variables.}
  \label{example}
\end{figure}

\section{Ancestral Graphs}

\section{d-seperation}

% This chapter, or series of chapters, delves into all technical details
% that are required to \emph{prove} your scientific hypothesis. It should
% be sufficiently detailed and precise in order for any fellow computing
% scientist student to be able to \emph{repeat} your research and
% therewith establish the same results / conclusions that you have
% obtained. Please note that, in order to improve readability of your
% thesis, you can put a part of this information also in one or more
% appendices (see Appendix \ref{appendix}).
\chapter{Algorithm Overview}\label{algorithm}
Our procedure turns out to be very simple, and we shall see that the most
challenging part is scoring the graph, which we get into in section
\ref{checking}. For now we shall give a short overview of the algorithm,
and then go into more detail on each of the problems. We give Python-esque
pseudocode:

% TODO(daan): improve pseudocode formatting
\begin{lstlisting}
def bccdgs(pag, statements):
    mag = pag_to_mag(pag)
    next_mag = next_mag(mag)

    while score(next_mag, statements) > score(mag, statements):
      mag = next_mag
      next_mag = next_mag(next_mag)

    return mag_to_pag(mag)
\end{lstlisting}
Where we use one helper function:
\begin{lstlisting}
def next_mag(mag, statements):
    adjacent_mags = adjacent_mags(mag)
    return best_scoring_mag(mags, statements)
\end{lstlisting}
We see that the first thing we do is convert the PAG into a MAG, since
scoring a MAG is easier. Then, within the while loop we keep looking for
the best adjacent mag with the next\_mag function. Within this function we
generate adjacent mags, and then pick the best scoring one. In short,
there are 4 main problems to solve:
\begin{enumerate}
  \item Transforming a PAG into a MAG.

  \item Transforming a MAG into a PAG.

  \item Generating adjacent MAGs.

  \item Scoring a MAG.
\end{enumerate}
We will cover 1-3 in this chapter, and scoring in chapter \ref{problem}.

\section{PAG to MAG}
To turn a PAG into a MAG we have to change all circle marks into either
tails or arrows. However, we must be careful that the MAG is a part of the
equivalence class. A proven correct method is as follows:

\begin{enumerate}
  \item Turn all semi-arcs into arcs. That is, turn
    $*\!\!\!\rightarrow$ into $\rightarrow$ and
    $*\!-$ into $\leftarrow$.

  \item Orient the remaining $*\!\!\!-\!\!\!*$ edges into a DAG with no new unshielded
    colliders.
\end{enumerate}
It is clear that all circle edges are either arrows or tails. For the
proof that this is a valid MAG from the correct equivalence class we refer
to the original paper\cite{zhangCompletenessOrientationRules2008}.

\section{MAG to PAG}
Turning a MAG back into a PAG is slightly more involved, as it is not
clear which marks must turn into circles. We do not have an algorithm that
works directly on a MAG, and instead we will use d-seperation to determine
independencies. We then feed this into the FCI algorithm, and make use of
its orientation rules. Since the rules are complete and sound, this will
create a PAG of the correct equivalence class.

\section{Adjacent MAGs}
\subsection{Generating}
This is the simplest problem to solve. We consider adjacent graphs to be
graphs which have one edge changed compared to the original. Given two
vertices $u$ and $v$, there are four possibilities:
\begin{align}
  u \rightarrow v \\
  u \leftarrow v \\
  u \leftrightarrow v\\
  (u, v) \notin E
\end{align}
Remembering that we ignored selection bias, and so there are no undirected
edges. Our original graph has one of these four. All adjacent MAGS can
then be easily generated by adding a graph which has one of the other 3
possibilities.

The real issue comes up when we want to check whether this MAG is also
valid; that is whether it has any almost directed cycles.

\subsection{Checking correctness}\label{transitive_closure}
We want to check that \emph{for any directed path from $u$ to $v$, there
does not exist an edge $u \leftarrow v$ or an edge $u \leftrightarrow v$.
} To do this, we will check a graph of all directed paths, the transitive
closure, and compare it to the original graph.

Although an efficient algorithm of $O(|V||E|)$ exists
\cite{purdomTransitiveClosureAlgorithm1970} and is implemented in the
boost graph library \cite{siekBoostGraphLibrary2002}, using this library
in Python is a bit of a hassle. Instead, we use a simple algorithm based
on matrix multiplication. This has $O(|V|^4)$, but the implementation is
optimized enough for our purposes, as we shall discuss in section
\ref{runtime}.

The definition of arbitrary matrix multiplication for $C = A \cdot B$ is:

\begin{equation*}
  c_{ij} = \sum ^n_{k=1} a_{ik}b_{kj}
\end{equation*}

Now for an adjacency matrix $A \in \mathbb{B}^{n \times n}$ the
self-product $A \cdot A$ becomes:

\begin{equation*}
  a^2_{ij} = \sum ^n_{k=1} a_{ik}a_{kj}
\end{equation*}

So we see that $a^2_{ij} > 0$ iff there is some directed path from $i$ to
$j$ of length 2. Now if we want the transitive closure we take $A^n$, as a
directed path is of maximum length $n$.

Now that we have the transitive closure we can easily check whether any
directed paths exist by comparing $A^n$ to $A$.

\section{Checking}\label{checking}
Although we build on the BCCD algorithm, it is quite complex, and details
on how it works are outside of the scope of this thesis. Instead, we will
focus on the output we use from it, which is the logical statements. For
the interested reader, we refer to the original paper on how they are
generated\cite{claassenBayesianApproachConstraint2012}.

Given that we ignore selection bias, the specific different logical
statements are as follows:
\begin{align*}
  X &\Rightarrow Y \\
  X \Rightarrow Y &\lor X \Rightarrow Z \\
  X \not \Rightarrow Y &\land X \not \Rightarrow Z \\
  (X, Y) &\notin E \\
  X &\ci Y
\end{align*}
We see that besides information about cause and non-cause, we now also
have information about the non-existence of an edge, and unconditional
independence of variables.

Besides the graph $G$ we will also need its transitive closure $G_{tc}$ to
check whether an arbitrary statement $s \in S$ is true. We use the
algorithm discussed in section \ref{transitive_closure}. Given this
transitive closure, checking (non-)cause and edge existence is incredibly
easy. For $X \Rightarrow Y$ we check $(X, Y) \in E_{tc}$, and $X \not
\Rightarrow Y$ similarly $(X, Y) \notin E_{tc}$. The only statement which
requires some work is unconditional independence.

% TODO(daan): have consistent notation for X -> .. -> Y
There are 3 cases in which $X \not \ci Y$:
\begin{enumerate}
  \item $(X, Y) \in E_{tc}$ or $(Y, X) \in E_{tc}$

  \item $X \leftrightarrow Y$

  \item There exists a $Z$ such that $X \leftarrow \ldots \leftarrow Z
    \rightarrow \ldots \rightarrow Y$
\end{enumerate}
These can all easily be checked with $G$ and $G_{tc}$.

\section{Scoring}
To create a concrete score from these statements we have to combine the
accuracies in some way. To do this we assume that these statements are
independent. This is patently untrue, but we need this assumption to
combine them easily.

% TODO(daan): fix notation of p_i
Now we can describe the probability of a graph $G$ given statements $S =
\{s_1, \ldots, s_l\}$ as follows:
\begin{equation*}
  \mathbb{P}(G | S) = \prod ^l_{i=1} p_i
\end{equation*}
Maximizing this is equivalent to maximizing the log-likelihood, which is
what we will do:
\begin{equation*}
  L(G|S) = \sum_{i=1}^l \log (p_i)
\end{equation*}
To find the best graph from a collection we then take the graph which
maximizes this score.

\section{Implementation details}
Now that all the pieces are there we give a quick overview of what we used
to implement them. We primarily used the Python package Numpy
\cite{harrisArrayProgrammingNumPy2020} to represent graphs as an adjacency
matrix. Besides that, the following packages were used:
\begin{itemize}
  \item Pcalg\cite{kalischCausalInferenceUsing2012} for pag2mag and fci
    functions. It is also used to generate random graphs, as we shall see
    in chapter \ref{results}.

  \item Scipy\cite{virtanenSciPyFundamentalAlgorithms2020} for the matrix
    power used in \ref{transitive_closure}.

  \item Seaborn\cite{waskomSeabornStatisticalData2021} and
    Matplotlib\cite{hunterMatplotlib2DGraphics2007} for plotting results.

  \item The rpy2 package to call R packages from Python.
\end{itemize}

\chapter{Results}\label{results}

\section{Graph size}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{lib/nodes_causal.pdf}
  \caption{Graph size}
  \label{nodes_causal}
\end{figure}

\section{Graph density}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{lib/density_causal.pdf}
  \caption{Density}
  \label{density_causal}
\end{figure}

\section{Skeleton}
\begin{figure}
  \centering
  \includegraphics[width=0.33\textwidth]{lib/skel_causal.pdf}
  \caption{Skeleton}
  \label{skel_causal}
\end{figure}

\section{Cutoff point}
\begin{figure}
  \centering
  \includegraphics[width=0.33\textwidth]{lib/cutoff_causal.pdf}
  \caption{Cutoff}
  \label{cutoff_causal}
\end{figure}

\section{Runtime}\label{runtime}
While the bccd portion, as well as the fci portion are both measured in
seconds, the greedy search is only several miliseconds.

% In this chapter you demonstrate that you are sufficiently aware of the
% state-of-art knowledge of the problem domain that you have investigated
% as well as demonstrating that you have found a \emph{new} solution /
% approach / method.
\chapter{Related Work}\label{relatedwork}
The most notable is
FCI\cite{spirtesCausationPredictionSearch2000}, which is a complete
\cite{zhangCompletenessOrientationRules2008} constrain based algorithm.


% In this chapter you present all conclusions that can be drawn from the
% preceding chapters. It should not introduce new experiments, theories,
% investigations, etc.: these should have been written down earlier in the
% thesis. Therefore, conclusions can be brief and to the point.
\chapter{Conclusions}\label{conclusions}

\printbibliography


% Appendices are \emph{optional} chapters in which you cover additional
% material that is required to support your hypothesis, experiments,
% measurements, conclusions, etc. that would otherwise clutter the
% presentation of your research.
% \appendix
% \chapter{Appendix}\label{appendix}

\end{document}
